{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3514c5cc",
   "metadata": {},
   "source": [
    "# Data Collector \n",
    "Для повышения модульности проект разбит на 2 блокнота - для сбора данных в датасет и собственно модель для работы с датасетом.  \n",
    "Этот блокнот содержит сборщик данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d0a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import nltk\n",
    "import nltk.tokenize as nt\n",
    "import nltk.corpus as nc\n",
    "import nltk.stem as ns\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "import pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89aa6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Leonid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d96f",
   "metadata": {},
   "source": [
    "Небольшой класс для разбора HTML, который встречается в субтитрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c8c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    text =\"\"\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.text+=data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc4326",
   "metadata": {},
   "source": [
    "### Словарь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b673bd",
   "metadata": {},
   "source": [
    "Так как операция создания словаря однократная , словари можно конвертировать из pdf в текст вручную\n",
    "Поэтому Словарь загружается из конверированных в текст PDF , каждый файл содержит словарные статьи с указанием частей речи.  \n",
    "Словарь должен содержать слова одной категории, категория указывается при загрузке файла\n",
    "Загруженный словарь содержит для каждого слова \n",
    "* множество категорий grade (сейчас используется всегда максимальная )\n",
    "* множество грамматических тэгов grammar ( сейчас не используется)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "#    part_of_speech = ['n.','v.','adj.','conj.','prep.','pron.']\n",
    "#    template = parts_of_speech.join('')\n",
    "    \n",
    "    def __init__(self, filenames):\n",
    "        self.words = {}\n",
    "        for fn,grade in filenames.items():\n",
    "            with open(fn) as f:\n",
    "                for ln in f:\n",
    "                    w,set_g = self.split_line(ln)\n",
    "                    self.insert(word = w,grade = grade, grammar= set_g )\n",
    "                            \n",
    "    def split_line(self,ln):\n",
    "        template='((n)|(v)|(prep)|(pron)|(conj)|(adj)|(adv))[\\.\\,]?'\n",
    "        tokens =ln.split(' ')\n",
    "        word = tokens[0]\n",
    "        gram = set()\n",
    "        for t in tokens[1:]:\n",
    "            m = re.match(template,t) \n",
    "            if m:\n",
    "                gram.add(m[1])\n",
    "        return word,gram\n",
    "                            \n",
    "    def insert(self, word, grade, grammar):\n",
    "        ww = self.words.pop(word,{})\n",
    "        gr = ww.pop('grammar',set())\n",
    "        gr |= grammar\n",
    "        ww['grammar'] =  gr\n",
    "        g = ww.pop('grade',set())\n",
    "        g.add(grade)\n",
    "        ww['grade']=g\n",
    "        self.words[word] = ww \n",
    "        \n",
    "    def summary(self):\n",
    "        count= 0\n",
    "        grades = {}\n",
    "        grammars = {}\n",
    "        for v in self.words.values():\n",
    "            if not str(v['grammar']) in grammars:\n",
    "                grammars[str(v['grammar'])] = 0 \n",
    "            grammars[str(v['grammar'])]+=1\n",
    "            if not str(v['grade']) in grades:\n",
    "                grades[str(v['grade'])] = 0 \n",
    "            grades[str(v['grade'])]+=1\n",
    "            count+=1    \n",
    "        return count,grades,grammars    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34575544",
   "metadata": {},
   "source": [
    "Простая обёртка для загрузки и чистки стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608f8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stops:\n",
    "    def __init__(self ):\n",
    "        self.stop_words = set(nc.stopwords.words(\"english\")) | {\"'\"}\n",
    "    \n",
    "    def purge(self,words): \n",
    "        return(  [ w for w in words if w.casefold() not in self.stop_words ] )\n",
    "\n",
    "stops = Stops()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae8748",
   "metadata": {},
   "source": [
    "Класс для сбора собственных имён, при анализе частота собственных имён является отдельным признаком "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22900c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NE_Chunks:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def extract(self,words):\n",
    "        tags = nltk.pos_tag(words)\n",
    "        tree = nltk.ne_chunk(tags, binary=True)\n",
    "        ne =  set(\n",
    "                \" \".join(i[0] for i in t)\n",
    "                for t in tree\n",
    "                if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "            )\n",
    "        common = words      \n",
    "        return(ne,common)\n",
    "    \n",
    "ne_chunks = NE_Chunks()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c757f2",
   "metadata": {},
   "source": [
    "На прикидках LancasterStemmer показал полное превосходство над остальными, не нашёл, в чём он может уступать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03645573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = SnowballStemmer('english')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb79742",
   "metadata": {},
   "source": [
    "### Сборщики статистики слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760faee4",
   "metadata": {},
   "source": [
    "Используются два класса для сбора статистики со всего файла  и с отдельного субтитра, они опираются на один базовый класс. Базовый сборщик подсчитывает количество слов каждого уровня(A1-С1) , количество собственных имен и количество всех слов во фрагменте, а также продолжительность фрагмента.  \n",
    "Для каждого уровня  подсчитываются как общее количество слов, так и количество раздельных слов в своем фрагменте текста   \n",
    "К сожалению,  порядка 30 % слов не попадает в подсчет (баг пока не найден)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa7beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubHandler:\n",
    "    \n",
    "    def tokenize(self,text):\n",
    "        rt = RegexpTokenizer(r\"\\w+|\\'\")\n",
    "        return( rt.tokenize(text) ) \n",
    "    \n",
    "    def parse(self):\n",
    "        self.tokens = stops.purge( self.tokenize(self.text) )\n",
    "        self.ne,self.tokens =  ne_chunks.extract(self.tokens)\n",
    "        self.tokens = [w.lower() for w in self.tokens ]\n",
    "        self.ne = {w.lower() for w in self.ne }           # ---> Named Entities\n",
    "        self.summary = nltk.FreqDist(self.tokens)         # ---> count entries for each word\n",
    "    \n",
    "    def classify(self,vocab):\n",
    "        def find_word_grade(w):                    # find a word in original or stemmed form and report its grade\n",
    "            gr = ''\n",
    "            try:\n",
    "                if w in vocab.words  : \n",
    "                    gr = max(vocab.words[w]['grade'])\n",
    "                elif  (stemmer.stem(w) in vocab.words):    \n",
    "                    gr= max(vocab.words[stemmer.stem(w)]['grade'])\n",
    "            except ValueError as ve: \n",
    "                print(w,ve)\n",
    "                \n",
    "            return(gr) \n",
    "    \n",
    "        def inc_dict_at( d,i,n):\n",
    "            g = d.get(i,0)\n",
    "            d[i] = g+n\n",
    "  \n",
    "                                      \n",
    "        self.nums = dict()\n",
    "        self.misses = {}\n",
    "        \n",
    "        g = ''\n",
    "        for w,n in self.summary.items():\n",
    "            # -- for each category , we accumulate general number of words with suffix _сnt\n",
    "            # -- and number of different word with _wrd\n",
    "            g = find_word_grade(w)\n",
    "            if bool(g):\n",
    "                inc_dict_at( self.nums,g+'_cnt',n)\n",
    "                inc_dict_at( self.nums,g+'_wrd',1)\n",
    "            elif w in self.ne:\n",
    "                inc_dict_at( self.nums,'ne_cnt',n) \n",
    "            else:    \n",
    "                self.misses[w]=n                                     \n",
    "            inc_dict_at( self.nums,'count',n)\n",
    "            inc_dict_at( self.nums,'length',len(w))\n",
    "  \n",
    "    def calc_freq(self):\n",
    "        # this method fills statistic by fragment , that must be reported to upper level\n",
    "        self.freq = self.nums.copy()\n",
    "        # first we put numbers of words by category\n",
    "        self.freq |= {'seconds':self.seconds()}\n",
    "        wc = len( self.summary ) \n",
    "        # also frequences relative to time \"_ps\" and to amount of differnt words \"_pw\"\n",
    "        self.freq |= { (k+'_pw'):round(v/wc,4) for (k,v) in self.nums.items() if k[-4:] in ['_cnt','_wrd'] }\n",
    "        self.freq |= { (k+'_ps'):round(v/self.seconds(),2) for (k,v) in self.nums.items() if k[-4:] in ['_cnt','_wrd']}\n",
    "\n",
    "    def statistic(self):\n",
    "        self.calc_freq()\n",
    "        return(self.freq )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce74004",
   "metadata": {},
   "source": [
    "#### Cборщик статистики по файлу\n",
    "Из базового класса наследует все функции по сбору статистики\n",
    "Кроме того, анализирует статистику по субтитрам, выбирая для каждого признака  средние и максимальные значения  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9c5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subfile(SubHandler):\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.text = ''\n",
    "        self.ne ={}\n",
    "        self.freq = dict()\n",
    "        self.summary = dict()\n",
    "        self.tokens = list()\n",
    "        self.misses = {}\n",
    "        file_name_parts = filename.split('/')[-1].split('\\\\')[-1].split('.')\n",
    "        self.movie = '.'.join( file_name_parts[:-1])                           # w/o 'srt'\n",
    "        self.load(filename)\n",
    "    \n",
    "    def load(self,filename):\n",
    "        encs = ['utf-8','iso-8859-1']\n",
    "        last_error = None\n",
    "        for en in encs:\n",
    "            try:\n",
    "                srt = pysrt.open(filename, encoding=en )\n",
    "            except UnicodeDecodeError as e:\n",
    "                last_error = e\n",
    "                srt = []\n",
    "            if srt :\n",
    "                break\n",
    "        \n",
    "        if not srt:\n",
    "            raise last_error \n",
    "\n",
    "        self.items = srt.read(filename)   \n",
    "        \n",
    "        h=MyHTMLParser()\n",
    "        h.feed(srt.text)\n",
    "        self.text = h.text  \n",
    "# -----------------------\n",
    "    def stat_by_of(self,agg,field):\n",
    "    # aggregator to calculate max or sum->average for feature (field)    \n",
    "        return( agg( st.get(field,0) for st in self.substatistics ) ) \n",
    "               \n",
    "    def seconds(self): \n",
    "    # for file, duration is time between first and last subtitres    \n",
    "        time_int = self.items[-1].end-self.items[0].start\n",
    "        return ( 3600*time_int.hours + 60*time_int.minutes + time_int.seconds + round(time_int.milliseconds/1000,3) )\n",
    "   \n",
    "\n",
    "    def classify_all(self,vocab):\n",
    "\n",
    "        self.substatistics = []               # -- storage to keep statistics for each subtitre\n",
    "        for it in self.items:\n",
    "            self.classify_one(it,vocab)\n",
    "               \n",
    "    def classify_one(self,it,vocab):  \n",
    "        sb = Subtitle(it)\n",
    "        if sb.seconds() <0.1:\n",
    "            return\n",
    "        sb.parse()\n",
    "        sb.classify(vocab)\n",
    "        st = sb.statistic()   \n",
    "        st = self.st0 | st      \n",
    "        self.substatistics.append(st)\n",
    " \n",
    "    def classify(self,vocab):\n",
    "    # first calucalate general stats as Subhandler, the analyze subtitres     \n",
    "        super().classify(vocab)\n",
    "        self.calc_freq()\n",
    "        self.st0 = { e:0 for (e,v) in self.freq.items() }        \n",
    "        self.classify_all(vocab)\n",
    "\n",
    "        \n",
    "    def statistic(self):\n",
    "        ss = len(self.substatistics)\n",
    "        maxs = { (k+'__max'):(self.stat_by_of(max,k)) for (k,v) in self.st0.items() } \n",
    "        means = { (k+'_mean'):(round(self.stat_by_of(sum,k)/ss,4)) for (k,v) in self.st0.items() }   \n",
    "        return({'movie':self.movie} | maxs | means |  self.freq )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba1932",
   "metadata": {},
   "source": [
    "#### Сборщик статистики по субтитру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b54733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtitle(SubHandler):\n",
    "    def __init__(self,item):\n",
    "        self.text = ''\n",
    "        self.ne ={}\n",
    "        self.freq = dict()\n",
    "        self.summary = dict()\n",
    "        self.tokens = list()\n",
    "        self.misses = {}\n",
    "\n",
    "        self.item = item\n",
    "        self.text = item.text\n",
    "        \n",
    "    def seconds(self): \n",
    "        time_int = self.item.end-self.item.start\n",
    "        return ( 3600*time_int.hours + 60*time_int.minutes + time_int.seconds + round(time_int.milliseconds/1000,3) )\n",
    "       \n",
    "    def statistic(self):\n",
    "        self.calc_freq()\n",
    "        return(self.freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb78e4c",
   "metadata": {},
   "source": [
    "#### Интегратор статистики по всем файлам\n",
    "Сохраняет результаты в датасет в каталоге dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6044d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "\n",
    "    def __init__(self): #, srt_pattern , scores_path)\n",
    "#         self.srt_pattern= srt_pattern\n",
    "#         self.xls_path = scores_path \n",
    "        self.process()\n",
    "    \n",
    "    def get_levels(self,xls_path):\n",
    "        df = pd.read_excel(xls_path)\n",
    "        df = df.drop('id',axis=1)\n",
    "        df.columns = ['movie','level'] \n",
    "        resolve_levels = {'A2/A2+':'A2' ,'A2/A2+':'A2','A2/A2+, B1':'A2','B1, B2':'B1' }\n",
    "        df['level'] = df.level.apply(lambda l: resolve_levels.get( l.strip(),l.strip() ) )\n",
    "        return df\n",
    "\n",
    "    def gather_stats(self,file_pattern,vocab):\n",
    "        def add_one(acc, new ):\n",
    "            for k in acc:\n",
    "                if k not in new:\n",
    "                    new[k]=0\n",
    "            for k,v in new.items():\n",
    "                if k in acc:\n",
    "                    acc[k].append(v)\n",
    "                else:\n",
    "                    acc[k] = [v]\n",
    "\n",
    "        acc = {}\n",
    "        # ---- parse file , considering possible usincide error -------\n",
    "        for fn in glob.glob(file_pattern):\n",
    "            try:\n",
    "                sf= Subfile(fn)        \n",
    "                sf.parse()\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(sf.movie,e)\n",
    "                continue\n",
    "            #  --- classify this and gather stats   \n",
    "            sf.classify(vocab = vocab)\n",
    "            st = sf.statistic()\n",
    "            add_one(acc,st)\n",
    " \n",
    "        return( pd.DataFrame.from_dict( acc ) ) \n",
    "    \n",
    "    def join_data(self, ds , df ):\n",
    "        dx = ds.merge(df,how='outer')\n",
    "        return(dx)\n",
    "        \n",
    "    def write(self, dx ):\n",
    "        dx.to_csv('datasets/movies.csv')\n",
    "\n",
    "            \n",
    "    def feature_cols(self,dx):\n",
    "        return( [c for c in dx.columns if c != 'movie'] )\n",
    "            \n",
    "    def process(self):\n",
    "        vocab =  Vocabulary({ 'Oxford_CEFR_level/A1.txt':'A1',\n",
    "                       'Oxford_CEFR_level/A2.txt':'A2',\n",
    "                      'Oxford_CEFR_level/B1.txt':'B1', \n",
    "                      'Oxford_CEFR_level/B2.txt':'B2',\n",
    "                      'Oxford_CEFR_level/C1.txt':'C1'   } ) \n",
    "        df = self.get_levels('English_scores/movies_labels.xlsx')\n",
    "        ds = self.gather_stats( 'English_scores/Subtitles_all/*/*.srt', vocab )\n",
    "        dx = self.join_data(ds,df)\n",
    "        self.write(dx)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab7895",
   "metadata": {},
   "source": [
    "### Run main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee6dbdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Downton Abbey - S01E07 - Episode 7.eng.SDH 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "CPU times: total: 10min 38s\n",
      "Wall time: 10min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DataCollector at 0x2b3f8a63d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DataCollector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bf334",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2385f",
   "metadata": {},
   "source": [
    "* Найти причину утечки слов мимо словаря \n",
    "* Найти утечки фильмов при join\n",
    "* Дать понятные наименования аттрибутам в Subhandle и производных классах\n",
    "* Проверить возможность анализа грамматических тэгов в словаре и грамм.раборе  для более точного определения уроня слова\n",
    "* Проводить очистку данных при выгрузке\n",
    "** Ввести отладочный параметр для выгрузки датафрейма with_na/witout_na\n",
    "** Находить корреляционные коллизии между признаками и удалять один из парв с корреляцией > 0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
